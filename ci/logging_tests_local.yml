---
#- name: Prepare Logging Tests
#  hosts: controller
#  gather_facts: true
#  ignore_errors: false
##  vars:
##    id_rsa_path:  "{{ playbook_dir }}/id_rsa.ctlplane"
##  environment:
##    KUBECONFIG: "{{ cifmw_openshift_kubeconfig }}"
##    PATH: "{{ cifmw_path }}"
#  vars_files:
#    - vars/common.yml
#  tasks:
#    - name: Create log dir
#      ansible.builtin.file:
#        path: "{{ logs_dir }}/logging_local"
#        state: directory
#        mode: "0755"

#    # This needs to be updated; what do we use elsewhere?
#    # Elsewhere, we assume that we're already logged in
#    - name: "Log into OCP"
#      ansible.builtin.shell:
#        cmd: |
#          oc login -u kubeadmin -p "12345678" https://api.crc.testing:6443
#      failed_when: false
#      changed_when: false
#
#    # I don't think we need this, Zuul __should__ have taken care of this already
#    - name: "Create id rsa file for ctlplane access"
#      ansible.builtin.shell:
#        cmd: |
#          oc extract -n openstack secrets/dataplane-ansible-ssh-private-key-secret --to=- | grep -v "ssh\-rsa" > "{{ id_rsa_path }}"
#      register: rsa_results
#      failed_when: rsa_results.rc != 0
#      changed_when: false
#
#    # (efoley): Once again, I don't think this is necessary; if it's not needed for zuul, because it's already done, but is needed before running this on other systems, then the README should be updated to reflect this;
#    # A troubleshooting section can be added too so users can identify when they are missing these configuration steps
#     - name: "Change files permissions"
#       ansible.builtin.shell:
#         cmd: |
#           chmod 600 "{{ id_rsa_path }}"
#       changed_when: false

- name: "Verify logging projects, endpoints, credentials, nodes, pods, services, manifests and subscriptions"
  hosts: controller
  gather_facts: no
  ignore_errors: true
  environment:
    KUBECONFIG: "{{ cifmw_openshift_kubeconfig }}"
    PATH: "{{ cifmw_path }}"
  vars:
    pod_test_id: "RHOS0-12672"
    pod_status_str: "Running"
    pod_nspace: openstack-operators
    pod_list:
      - telemetry-operator-controller-manager
      - dataplane-operator-controller-manager

- name: "Verify logging pods are running in openstack"
  hosts: controller
  gather_facts: no
  ignore_errors: true
  environment:
    KUBECONFIG: "{{ cifmw_openshift_kubeconfig }}"
    PATH: "{{ cifmw_path }}"
  vars:
    pod_test_id: "RHOSO-12752"
    pod_status_str: "Running"
    pod_nspace: openstack
    pod_list:
      - openstackclient
  tasks:
    - name: "Verify Running Pods"
      ansible.builtin.import_role:
        name: common


- name: "Verify logging pods are running in openshift-operators-redhat"
  hosts: controller
  gather_facts: no
  ignore_errors: true
  environment:
    KUBECONFIG: "{{ cifmw_openshift_kubeconfig }}"
    PATH: "{{ cifmw_path }}"
  vars:
    pod_test_id: "RHOSO-12673"
    pod_status_str: "Running"
    pod_nspace: openshift-operators-redhat
    pod_list:
      - loki-operator-controller-manager

  tasks:
    - name: "Verify Pods running"
      ansible.builtin.import_role:
        name: common
        # vars can be passed here, so we can use the same play for this and the next play.
      # vars:
      #   - <vars from above>

- name: "Verify logging pods are running in openshift-logging"
  hosts: controller
  gather_facts: no
  ignore_errors: true
  environment:
    KUBECONFIG: "{{ cifmw_openshift_kubeconfig }}"
    PATH: "{{ cifmw_path }}"
  vars:
    pod_test_id: "RHOSO-12676"
    pod_status_str: "Running"
    pod_nspace: openshift-logging
    pod_list:
      - cluster-logging-operator
      - collector
      - logging-loki-compactor
      - logging-loki-distributor
      - logging-loki-gateway
      - logging-loki-index-gateway
      - logging-loki-ingester
      - logging-loki-querier
      - logging-loki-query-frontend
      - logging-view-plugin

  ### see JIRA LOG-5431 if pods not running
  tasks:
    - name: "Verify Pods running" 
      ansible.builtin.import_role:
        name: common

- name: "Verify logging pods are running in minio-dev"
  hosts: controller
  gather_facts: no
  ignore_errors: true
  environment:
    KUBECONFIG: "{{ cifmw_openshift_kubeconfig }}"
    PATH: "{{ cifmw_path }}"
  vars:
    pod_test_id: "RHOSO-12674"
    pod_status_str: "Running"
    pod_nspace: minio-dev
    pod_list:
      - minio

  tasks:
    - name: "Run pod running tests"
      ansible.builtin.import_role:
        name: common


- name: "Verify logging pods have complete status in openstack"
  hosts: controller
  gather_facts: no
  ignore_errors: true
  environment:
    KUBECONFIG: "{{ cifmw_openshift_kubeconfig }}"
    PATH: "{{ cifmw_path }}"
  vars:
    pod_test_id: "RHOSO-12679"
    pod_nspace: openstack
    pod_status_str: "Completed"
    pod_list:
      - bootstrap-edpm-deployment-logging
      - configure-network-edpm-deployment-logging
      - configure-os-edpm-deployment-logging
      - download-cache-edpm-deployment-logging
      - install-certs-edpm-deployment-logging
      - install-os-edpm-deployment-logging
      - libvirt-edpm-deployment-logging
      - logging-edpm-deployment-logging-openstack-edpm
      - neutron-metadata-edpm-deployment-logging
      - nova-custom-edpm-deployment
      - ovn-edpm-deployment-logging
      - reboot-os-edpm-deployment-logging
      - repo-setup-edpm-deployment-logging
      - run-os-edpm-deployment-logging
      - ssh-known-hosts-edpm-deployment-logging
      - telemetry-edpm-deployment-logging
      - validate-network-edpm-deployment-logging
  tasks:
    - name: "Run pods completed tests"
      ansible.builtin.import_role:
        name: common

